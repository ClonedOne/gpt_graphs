{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import bfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In context learning prompts\n",
    "\n",
    "In this notebook we will create a set of prompts to test the in context learning capabilities of ChatGPT when applied to graph visualization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank assignment prompts\n",
    "\n",
    "We will start with the problem of rank assignment. We will generate the new prompts by randomly sampling 5 graphs from different files, and prepending their correct solutions to the prompt asking for the solution of the current graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"\"\"\n",
    "Perform a rank assignment on the graph. Use node 0 as a source for the graph. Each node must be assigned to a rank that is equal to the shortest path between that node and the source. Thus, node 0 will be assigned to rank 0, and the neighbors of node 0 will be assigned to rank 1. \n",
    "Write no explanations, only respond with the id of each node and the rank it has been assigned to in a format <id> - <rank>.\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompt_query_dir = \"queries/rank_prompts\"\n",
    "all_rank_prompt_files = set(os.listdir(rank_prompt_query_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_edge_list(query: str) -> list:\n",
    "    rank_prompt_edge_list = (\n",
    "        query.split(\"edge connections:\")[1]\n",
    "        .split(\"Perform a rank assignment\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    rank_prompt_edge_list = list(ast.literal_eval(rank_prompt_edge_list))\n",
    "    return rank_prompt_edge_list\n",
    "\n",
    "def rank_assignment_to_formatted_str(rank_assignment: dict) -> str:\n",
    "    rank_assignment_str = \"\"\n",
    "    for rank, nodes in rank_assignment.items():\n",
    "        for node in nodes:\n",
    "            rank_assignment_str += f\"{node} - {rank}\\n\"\n",
    "    return rank_assignment_str\n",
    "\n",
    "def minimize_prompt(query: str) -> str:\n",
    "    query = query.split(\"\\n\")\n",
    "    query = query[:-2]\n",
    "    query = \"\\n\".join(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 queries\n"
     ]
    }
   ],
   "source": [
    "k_samples = 3\n",
    "sample_size = 100\n",
    "\n",
    "# Sample `sample_size` queries from the all_rank_prompt_files\n",
    "sampled_rank_prompt_files = np.random.choice(\n",
    "    list(all_rank_prompt_files), sample_size, replace=False\n",
    ")\n",
    "print(f\"Sampled {len(sampled_rank_prompt_files)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompts_icl = {}\n",
    "\n",
    "for rank_prompt_file in sampled_rank_prompt_files:\n",
    "    # Sample 5 other prompt files different from the current one\n",
    "    other_rank_prompt_files = np.random.choice(\n",
    "        list(all_rank_prompt_files - {rank_prompt_file}), k_samples, replace=False\n",
    "    )\n",
    "    assert rank_prompt_file not in other_rank_prompt_files\n",
    "\n",
    "    # Read the current prompt and the other prompts\n",
    "    rank_prompt = open(os.path.join(rank_prompt_query_dir, rank_prompt_file)).read()\n",
    "    other_rank_prompts = [\n",
    "        open(os.path.join(rank_prompt_query_dir, other_rank_prompt_file)).read()\n",
    "        for other_rank_prompt_file in other_rank_prompt_files\n",
    "    ]\n",
    "\n",
    "    # Extract the edge list for both the current prompt and the other prompts\n",
    "    rank_prompt_edge_list = read_prompt_edge_list(rank_prompt)\n",
    "    other_rank_prompt_edge_lists = [\n",
    "        read_prompt_edge_list(other_rank_prompt)\n",
    "        for other_rank_prompt in other_rank_prompts\n",
    "    ]\n",
    "\n",
    "    # Compute the correct rank assignment for the other prompts\n",
    "    other_rank_prompt_rank_assignments = [\n",
    "        bfs(other_rank_prompt_edge_list, 0)\n",
    "        for other_rank_prompt_edge_list in other_rank_prompt_edge_lists\n",
    "    ]\n",
    "\n",
    "    # Convert the rank assignments in the expected format, i.e. <id> - <rank> one per row\n",
    "    other_rank_prompt_rank_assignments_str = [\n",
    "        rank_assignment_to_formatted_str(other_rank_prompt_rank_assignment)\n",
    "        for other_rank_prompt_rank_assignment in other_rank_prompt_rank_assignments\n",
    "    ]\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = [\n",
    "        \"Input:\\n{}\\nAnswer:\\n{}\\n\".format(\n",
    "            minimize_prompt(other_rank_prompts[i]),\n",
    "            other_rank_prompt_rank_assignments_str[i],\n",
    "        )\n",
    "        for i in range(k_samples)\n",
    "    ]\n",
    "    prompt = prompt_base + \"\".join(prompt)\n",
    "    prompt += \"Input:\\n{}\\nAnswer:\\n\".format(minimize_prompt(rank_prompt))\n",
    "\n",
    "    # expected_answer = bfs(rank_prompt_edge_list, 0)\n",
    "    # expected_answer_str = rank_assignment_to_formatted_str(expected_answer)\n",
    "\n",
    "    rank_prompts_icl[rank_prompt_file] = prompt.strip()\n",
    "\n",
    "    del rank_prompt, other_rank_prompts, prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompts_icl_query_dir = \"queries/rank_prompts_icl\"\n",
    "os.makedirs(rank_prompts_icl_query_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompts to disk\n",
    "for rank_prompt_file, prompt in rank_prompts_icl.items():\n",
    "    with open(os.path.join(rank_prompts_icl_query_dir, rank_prompt_file), \"w\") as f:\n",
    "        f.write(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose prompts\n",
    "\n",
    "We can repeat a similar process for the crossing minimization prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"\"\"We want to reduce crossings on a graph drawing.\n",
    "We want to order the nodes in the layers so that there are few crossings in the graph. \n",
    "- visit every rank once, starting from layer 0\n",
    "- try different transpositions of the nodes in that layer\n",
    "- count the crossings for every transposition. There is a crossing between two edges e1 and e2 if the source of e1 comes before the source of e2, and the target of e1 comes after the target of e2 in the order of nodes in a layer.\n",
    "- record the transposition that produces the least amount of crossings, and sort the nodes accordingly.\n",
    "Nodes can NOT be moved to a different layer. You can only reorder nodes within layers.\n",
    "Write no code and no explanation.\n",
    "Return the layers dictionary with the nodes ordered, in a code block. I want it formatted like this: {<layer_id>:[<list of ordered nodes>]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_query_dir = \"queries/transpose_prompts3\"\n",
    "all_transpose_files = set(os.listdir(transpose_query_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 queries\n"
     ]
    }
   ],
   "source": [
    "k_samples = 3\n",
    "sample_size = 100\n",
    "\n",
    "# Sample `sample_size` queries from the all_rank_prompt_files\n",
    "sampled_transpose_files = np.random.choice(\n",
    "    list(all_transpose_files), sample_size, replace=False\n",
    ")\n",
    "print(f\"Sampled {len(sampled_transpose_files)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries:  134\n"
     ]
    }
   ],
   "source": [
    "transpose_queries = {}\n",
    "\n",
    "for query_file in sorted(os.listdir(transpose_query_dir)):\n",
    "    transpose_queries[query_file] = {}\n",
    "    query_file_path = os.path.join(transpose_query_dir, query_file)\n",
    "\n",
    "    query_str = open(query_file_path, \"r\").read().strip()\n",
    "\n",
    "    query_edges = query_str.split(\"edges = \")[1]\n",
    "    query_edges = query_edges.split(\"\\n\")[0].strip()\n",
    "    query_edges = ast.literal_eval(query_edges)\n",
    "\n",
    "    query_ranks = query_str.split(\"ranks = \")[1]\n",
    "    query_ranks = query_ranks.split(\"\\n\\n\")[0].strip()\n",
    "    query_ranks = query_ranks.split(\"\\n\")\n",
    "    # From each substring remove \"Layer \" at the \n",
    "    # beginning and add \",\" at the end\n",
    "    query_ranks = [r[6:].strip() + \",\" for r in query_ranks]\n",
    "    query_ranks = \"\".join(query_ranks)\n",
    "    query_ranks = \"{\" + query_ranks[:-1] + \"}\"\n",
    "    query_ranks = ast.literal_eval(query_ranks)\n",
    "    \n",
    "    transpose_queries[query_file][\"edges\"] = query_edges\n",
    "    transpose_queries[query_file][\"ranks\"] = query_ranks\n",
    "\n",
    "print(\"Number of queries: \", len(transpose_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query_to_str(edges: list, ranks: dict) -> str:\n",
    "    \"\"\"Format a query to a string\"\"\"\n",
    "    query = []\n",
    "    query.append(\"This is the list of edges. Every edge has [<source_id>, <target_id>]:\")\n",
    "    query.append(\"edges = {}\".format(edges))\n",
    "    query.append(\"This is the description of what nodes are contained in what layer: \")\n",
    "    for layer, nodes in ranks.items():\n",
    "        if layer == 0:\n",
    "            query.append(\"ranks = Layer {}: {}\".format(layer, nodes))\n",
    "        else:\n",
    "            query.append(\"Layer {}: {}\".format(layer, nodes))\n",
    "    query = \"\\n\".join(query)\n",
    "    return query\n",
    "\n",
    "def format_ground_truth(ranks: dict) -> str:\n",
    "    \"\"\"Format the ground truth to a string\"\"\"\n",
    "    gt = {int(k): v for k, v in ranks.items()}\n",
    "    \n",
    "    ground_truth = \"{\\n\"\n",
    "    for layer, nodes in gt.items():\n",
    "        ground_truth += f\"{layer}: {nodes},\\n\"\n",
    "    ground_truth = ground_truth[:-2] + \"\\n}\"\n",
    "    return ground_truth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries with ground truth:  207\n"
     ]
    }
   ],
   "source": [
    "# Let's load the ground truth answers for the minimal number of crossings for each query\n",
    "strasifimal_results = json.load(open(\"stratisfimal_results/optimal_ranks_10_11.json\", \"r\"))\n",
    "print(\"Number of queries with ground truth: \", len(strasifimal_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_icl = {}\n",
    "\n",
    "for transpose_file in sampled_transpose_files:\n",
    "    # Sample 5 other prompt files different from the current one\n",
    "    other_transpose_files = np.random.choice(\n",
    "        list(all_transpose_files - {transpose_file}), k_samples, replace=False\n",
    "    )\n",
    "    assert transpose_file not in other_transpose_files\n",
    "\n",
    "    cur_edges = transpose_queries[transpose_file][\"edges\"]\n",
    "    cur_ranks = transpose_queries[transpose_file][\"ranks\"]\n",
    "    other_edges = [\n",
    "        transpose_queries[other_file][\"edges\"] for other_file in other_transpose_files\n",
    "    ]\n",
    "    other_ranks = [\n",
    "        transpose_queries[other_file][\"ranks\"] for other_file in other_transpose_files\n",
    "    ]\n",
    "\n",
    "    ground_truth = strasifimal_results[transpose_file.split(\".txt\")[0]]\n",
    "    other_ground_truth = [\n",
    "        strasifimal_results[other_file.split(\".txt\")[0]]\n",
    "        for other_file in other_transpose_files\n",
    "    ]\n",
    "\n",
    "    cur_query = format_query_to_str(cur_edges, cur_ranks)\n",
    "    other_queries = [\n",
    "        format_query_to_str(other_edges[i], other_ranks[i]) for i in range(k_samples)\n",
    "    ]\n",
    "    other_ground_truth_str = [format_ground_truth(gt) for gt in other_ground_truth]\n",
    "\n",
    "    # print(\"Current query: \", cur_edges, cur_ranks)\n",
    "    # print(\"Other queries: \", other_edges, other_ranks)\n",
    "\n",
    "    # print(format_query_to_str(cur_edges, cur_ranks))\n",
    "    # print(\"Ground truth: \", ground_truth)\n",
    "    # print(format_ground_truth(ground_truth))\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = [\n",
    "        \"\\nInput:\\n{}\\nAnswer:\\n\\n{}\\n\".format(\n",
    "            other_queries[i],\n",
    "            other_ground_truth_str[i],\n",
    "        )\n",
    "        for i in range(k_samples)\n",
    "    ]\n",
    "    prompt = prompt_base + \"\\n\" + \"\".join(prompt)\n",
    "    prompt += \"\\nInput:\\n{}\\n\\nAnswer:\\n\".format(cur_query)\n",
    "\n",
    "    transpose_icl[transpose_file] = prompt.strip()\n",
    "    del (\n",
    "        cur_edges,\n",
    "        cur_ranks,\n",
    "        other_edges,\n",
    "        other_ranks,\n",
    "        ground_truth,\n",
    "        other_ground_truth,\n",
    "        prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tojs = [i.split(\".txt\")[0].strip() for i in sorted(list(all_transpose_files))]\n",
    "# json.dump(tojs, open(\"transpose_prompts3_files.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_prompts_icl_query_dir = \"queries/transpose_prompts_icl\"\n",
    "os.makedirs(transpose_prompts_icl_query_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompts to disk\n",
    "for query_file, prompt in transpose_icl.items():\n",
    "    query_file_path = os.path.join(transpose_prompts_icl_query_dir, query_file)\n",
    "    with open(query_file_path, \"w\") as f:\n",
    "        f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dafb023fb61a9cb21231b21edf636d8c395a3f8533b033bfdca443941bcc4563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
