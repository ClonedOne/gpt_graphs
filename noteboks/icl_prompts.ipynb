{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import bfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In context learning prompts\n",
    "\n",
    "In this notebook we will create a set of prompts to test the in context learning capabilities of ChatGPT when applied to graph visualization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank assignment prompts\n",
    "\n",
    "We will start with the problem of rank assignment. We will generate the new prompts by randomly sampling 5 graphs from different files, and prepending their correct solutions to the prompt asking for the solution of the current graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"\"\"\n",
    "Perform a rank assignment on the graph. Use node 0 as a source for the graph. Each node must be assigned to a rank that is equal to the shortest path between that node and the source. Thus, node 0 will be assigned to rank 0, and the neighbors of node 0 will be assigned to rank 1. \n",
    "Write no explanations, only respond with the id of each node and the rank it has been assigned to in a format <id> - <rank>.\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompt_query_dir = \"queries/rank_prompts\"\n",
    "all_rank_prompt_files = set(os.listdir(rank_prompt_query_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_edge_list(query: str) -> list:\n",
    "    rank_prompt_edge_list = (\n",
    "        query.split(\"edge connections:\")[1]\n",
    "        .split(\"Perform a rank assignment\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    rank_prompt_edge_list = list(ast.literal_eval(rank_prompt_edge_list))\n",
    "    return rank_prompt_edge_list\n",
    "\n",
    "def rank_assignment_to_formatted_str(rank_assignment: dict) -> str:\n",
    "    rank_assignment_str = \"\"\n",
    "    for rank, nodes in rank_assignment.items():\n",
    "        for node in nodes:\n",
    "            rank_assignment_str += f\"{node} - {rank}\\n\"\n",
    "    return rank_assignment_str\n",
    "\n",
    "def minimize_prompt(query: str) -> str:\n",
    "    query = query.split(\"\\n\")\n",
    "    query = query[:-2]\n",
    "    query = \"\\n\".join(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 queries\n"
     ]
    }
   ],
   "source": [
    "k_samples = 3\n",
    "sample_size = 100\n",
    "\n",
    "# Sample `sample_size` queries from the all_rank_prompt_files\n",
    "sampled_rank_prompt_files = np.random.choice(\n",
    "    list(all_rank_prompt_files), sample_size, replace=False\n",
    ")\n",
    "print(f\"Sampled {len(sampled_rank_prompt_files)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompts_icl = {}\n",
    "\n",
    "for rank_prompt_file in sampled_rank_prompt_files:\n",
    "    # Sample 5 other prompt files different from the current one\n",
    "    other_rank_prompt_files = np.random.choice(\n",
    "        list(all_rank_prompt_files - {rank_prompt_file}), k_samples, replace=False\n",
    "    )\n",
    "    assert rank_prompt_file not in other_rank_prompt_files\n",
    "\n",
    "    # Read the current prompt and the other prompts\n",
    "    rank_prompt = open(os.path.join(rank_prompt_query_dir, rank_prompt_file)).read()\n",
    "    other_rank_prompts = [\n",
    "        open(os.path.join(rank_prompt_query_dir, other_rank_prompt_file)).read()\n",
    "        for other_rank_prompt_file in other_rank_prompt_files\n",
    "    ]\n",
    "\n",
    "    # Extract the edge list for both the current prompt and the other prompts\n",
    "    rank_prompt_edge_list = read_prompt_edge_list(rank_prompt)\n",
    "    other_rank_prompt_edge_lists = [\n",
    "        read_prompt_edge_list(other_rank_prompt)\n",
    "        for other_rank_prompt in other_rank_prompts\n",
    "    ]\n",
    "\n",
    "    # Compute the correct rank assignment for the other prompts\n",
    "    other_rank_prompt_rank_assignments = [\n",
    "        bfs(other_rank_prompt_edge_list, 0)\n",
    "        for other_rank_prompt_edge_list in other_rank_prompt_edge_lists\n",
    "    ]\n",
    "\n",
    "    # Convert the rank assignments in the expected format, i.e. <id> - <rank> one per row\n",
    "    other_rank_prompt_rank_assignments_str = [\n",
    "        rank_assignment_to_formatted_str(other_rank_prompt_rank_assignment)\n",
    "        for other_rank_prompt_rank_assignment in other_rank_prompt_rank_assignments\n",
    "    ]\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = [\n",
    "        \"Input:\\n{}\\nAnswer:\\n{}\\n\".format(\n",
    "            minimize_prompt(other_rank_prompts[i]),\n",
    "            other_rank_prompt_rank_assignments_str[i],\n",
    "        )\n",
    "        for i in range(k_samples)\n",
    "    ]\n",
    "    prompt = prompt_base + \"\".join(prompt)\n",
    "    prompt += \"Input:\\n{}\\nAnswer:\\n\".format(minimize_prompt(rank_prompt))\n",
    "\n",
    "    # expected_answer = bfs(rank_prompt_edge_list, 0)\n",
    "    # expected_answer_str = rank_assignment_to_formatted_str(expected_answer)\n",
    "\n",
    "    rank_prompts_icl[rank_prompt_file] = prompt.strip()\n",
    "\n",
    "    del rank_prompt, other_rank_prompts, prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_prompts_icl_query_dir = \"queries/rank_prompts_icl\"\n",
    "os.makedirs(rank_prompts_icl_query_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompts to disk\n",
    "for rank_prompt_file, prompt in rank_prompts_icl.items():\n",
    "    with open(os.path.join(rank_prompts_icl_query_dir, rank_prompt_file), \"w\") as f:\n",
    "        f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dafb023fb61a9cb21231b21edf636d8c395a3f8533b033bfdca443941bcc4563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
